{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb67ba8d",
   "metadata": {},
   "source": [
    "# NLU: Mid-Term Assignment 2022\n",
    "### Description\n",
    "In this notebook, we ask you to complete four main tasks to show what you have learnt during the NLU labs. Therefore, to complete the assignment please refer to the concepts, libraries and other materials shown and used during the labs. The last task is not mandatory, it is a *BONUS* to get an extra mark for the laude. \n",
    "\n",
    "### Instructions\n",
    "- **Dataset**: in this notebook, you are asked to work with the dataset *Conll 2003* provided by us in the *data* folder. Please, load the files from the *data* folder and **do not** change names or paths of the inner files. \n",
    "- **Output**: for each part of your task, print your results and leave it in the notebook. Please, **do not** send a jupyter notebook without the printed outputs.\n",
    "- **Other**: follow carefully all the further instructions and suggestions given in the question descriptions.\n",
    "\n",
    "### Deadline\n",
    "The deadline is due in two weeks from the project presentation. Please, refer to *piazza* channel for the exact date."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996d123d",
   "metadata": {},
   "source": [
    "### Task 1: Analysis of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ead0d1f",
   "metadata": {},
   "source": [
    "#### Q 1.1\n",
    "- Create the Vocabulary and Frequency Dictionary of the:\n",
    "    1. Whole dataset\n",
    "    2. Train set\n",
    "    3. Test set\n",
    "    \n",
    "**Attention**: print the first 20 words of the Dictionaty of each set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41abee67",
   "metadata": {},
   "source": [
    "##### <p style='color:lightskyblue'> Q 1.1 Development </p>\n",
    "<p style='color:lightskyblue'>To produce a Vocabulary and a Frequency Dictionary we will use the <code>nltk</code> and <code>spaCy</code> libraries.</p>\n",
    "<p style='color:lightskyblue'>The former is used to load <a href=\"data/test.txt\">Test</a>, <a href=\"data/train.txt\">Train</a>, and <a href=\"data/valid.txt\">Validation</a> datasets.</p>\n",
    "\n",
    "<p style='color:lightskyblue'>\n",
    "    For the <b>tokenization</b> task we use the <code>nltk.word_tokenize</code> method. This will allow us to get tokens (words) out of the raw text.\n",
    "    However, in order to create a vocabulary, we iterate through tokens that have been set to lowercase, and add them to a <code>set</code> resulting in a list-like object but with unique values in it.\n",
    "</p>\n",
    "<p style='color:lightskyblue'>\n",
    "    For the <b>frequency</b> task we use the <code>nltk.FreqDist</code> method. This will allow us to get a dictionary representing the frequency with which each word appears in the input text. Secondly, we display the <i>top-20</i> words in terms of frequency for each text file using the. To do this we use a modified version of the <code>nbest</code> function provided in the first lab session, available in the <a href=\"utils.py\">utils</a> file.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca1124f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import nltk\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# Stopwords\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as SPACY_STOP_WORDS\n",
    "\n",
    "from utils import nbest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d73dc2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# IMPORTING DATASETS\n",
    "raw_test = nltk.load('./data/test.txt')\n",
    "raw_train = nltk.load('./data/train.txt')\n",
    "raw_val = nltk.load('./data/valid.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02218bf",
   "metadata": {},
   "source": [
    "<p style='color:lightskyblue'>Test</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d570e1cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['franco',\n",
       " 'spots',\n",
       " 'advance',\n",
       " 'kong',\n",
       " 'affecting',\n",
       " 'try',\n",
       " 'subject',\n",
       " 'largest',\n",
       " 'first-class',\n",
       " 'north',\n",
       " 'morale-boosting',\n",
       " 'safe',\n",
       " 'waited',\n",
       " 'urska',\n",
       " 'above',\n",
       " 'iseas',\n",
       " 'megan',\n",
       " 'sergio',\n",
       " 'hindu',\n",
       " 'start']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization\n",
    "test_words = nltk.word_tokenize(raw_test)\n",
    "\n",
    "test_vocabulary = set([ word.lower() for word in test_words])\n",
    "# printing first 20 words in test Vocab\n",
    "list(test_vocabulary)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac428d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency Dictionary from raw text\n",
    "test_freq_dict = nltk.FreqDist(test_words)\n",
    "\n",
    "top_20_test = nbest(test_freq_dict, n=20)\n",
    "top_20_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02218bf",
   "metadata": {},
   "source": [
    "<p style='color:lightskyblue'>Train</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d570e1cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['employer',\n",
       " 'tutsis',\n",
       " '25-30',\n",
       " 'reid',\n",
       " 'peso',\n",
       " 'uag',\n",
       " 'col',\n",
       " 'housing',\n",
       " 'detain',\n",
       " 'volgograd',\n",
       " '369.00',\n",
       " 'as-safir',\n",
       " 'watkinson',\n",
       " 'budge',\n",
       " 'kapoor',\n",
       " 'please',\n",
       " 'jonathon',\n",
       " 'sutjeska',\n",
       " 'steptoe',\n",
       " 'lasted']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TRAIN\n",
    "\n",
    "# Tokenization\n",
    "train_words = nltk.word_tokenize(raw_train)\n",
    "\n",
    "train_vocab = set([ word.lower() for word in train_words])\n",
    "# printing first 20 words in train Vocab\n",
    "list(train_vocab)[:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad011ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency Dictionary from raw text\n",
    "train_freq_dict = nltk.FreqDist(train_words)\n",
    "\n",
    "top_20_train = nbest(train_freq_dict, n=20)\n",
    "top_20_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02218bf",
   "metadata": {},
   "source": [
    "<p style='color:lightskyblue'>Validation</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d570e1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALIDATION\n",
    "\n",
    "# Tokenization\n",
    "val_sents = nltk.sent_tokenize(raw_val)\n",
    "val_words = nltk.word_tokenize(raw_val)\n",
    "\n",
    "val_vocab = set([ word.lower() for word in val_words])\n",
    "# printing first 20 words in val Vocab\n",
    "list(val_vocab)[:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52750c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency Dictionary from raw text\n",
    "val_freq_dict = nltk.FreqDist(val_words)\n",
    "\n",
    "top_20_val = nbest(val_freq_dict, n=20)\n",
    "val_freq_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becdc520",
   "metadata": {},
   "source": [
    "<p style=\"color:lightskyblue\">Here we repeat the same <b>tokenization</b> and <b>frequency-list</b> tasks using the <code>spaCy</code> library.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09e0588",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# USING SPACY\n",
    "\n",
    "# Tokenization using SpaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "test_words = nlp(raw_test)\n",
    "test_vocab = set([ token.text for token in test_words ])\n",
    "list(test_vocab)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c3d3e08f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "821087"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization using SpaCy\n",
    "train_words = nlp(raw_train)\n",
    "train_vocab = set([ token.text for token in train_words])\n",
    "list(train_vocab)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0dc02f",
   "metadata": {},
   "source": [
    "#### Q 1.2\n",
    "- Obtain the list of:\n",
    "    1. Out-Of-Vocabulary (OOV) tokens\n",
    "    2. Overlapping tokens between train and test sets  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b630aa7",
   "metadata": {},
   "source": [
    "##### <h style='color:lightskyblue'> Q 1.2 Development</h>\n",
    "<p style='color:lightskyblue'>\n",
    "    We consider as <strong>OOVs</strong> characters such as <em>punctuation</em> and <em>words containing numbers</em>.\n",
    "    For convenience, both <code>spaCy</code> and <code>nltk</code> offer a list of stopwords which is displayed in the cell below.\n",
    "</p>\n",
    "<p style='color:lightskyblue'>\n",
    "    In the cells above, we tried to use spacy's <code>.is_oov</code> attribute to tokens, but the result is ambiguos in the sense that words like <code>-DOCSTART-</code>, <code>-X-</code>, <code>NN</code>, etc are not meaningfull hance not desirable training inputs. However, such an attribute also removes meaningful words such as <code>SOCCER</code>, <code>JAPAN</code>, etc.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cefb6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Lists of Stopwords\n",
    "NLTK_STOP_WORDS = set(stopwords.words('english'))\n",
    "SPACY_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5d928b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Overlap between Test and Train sets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab1ac1c",
   "metadata": {},
   "source": [
    "#### Q 1.3\n",
    "- Perform a complete data analysis of the whole dataset (train + test sets) to obtain:\n",
    "    1. Average sentence length computed in number of tokens\n",
    "    2. The 50 most-common tokens\n",
    "    3. Number of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36e5c39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading sentences\n",
    "def load_as_sents(path):\n",
    "    sents = []\n",
    "    with open(path, 'r') as f:\n",
    "        [sents.append(line.strip()) for line in f.readlines()]\n",
    "    return sents\n",
    "\n",
    "test_sents = load_as_sents('./data/test.txt')\n",
    "train_sents = load_as_sents('./data/train.txt')\n",
    "dataset = train_sents + test_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9be2c371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.79"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average sentence length computed in number of tokens \n",
    "from statistics import mean\n",
    "def tokens_in_sent(text_sents):\n",
    "    '''\n",
    "    Computes the average sentence length in terms of tokens per sentence of the given :param text_sents: .\n",
    "    Params:\n",
    "    - :param text_sents: list of sentences.\n",
    "    Returns:\n",
    "    - :sent_len: number of tokens per element of the :param text_sents: list. \n",
    "    '''\n",
    "    sent_len = []\n",
    "    for sent in text_sents:\n",
    "        sent_ = sent.split(' ')\n",
    "        sent_len.append(len(sent_))\n",
    "    return mean(sent_len)\n",
    "\n",
    "words_per_sent = tokens_in_sent(dataset)\n",
    "round(words_per_sent,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fa9e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 50 most-common tokens\n",
    "top_20_val = nbest(val_freq_dict, n=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9b5ba6b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "269904"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of sentences\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726af097",
   "metadata": {},
   "source": [
    "#### Q 1.4\n",
    "- Create the dictionary of Named Entities and their Frequencies for the:\n",
    "    1. Whole dataset\n",
    "    2. Train set\n",
    "    3. Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5659670d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19a08f37",
   "metadata": {},
   "source": [
    "### Task 2: Working with Dependecy Tree\n",
    "*Suggestions: use Spacy pipeline to retreive the Dependecy Tree*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1ba597",
   "metadata": {},
   "source": [
    "#### Q 2.1\n",
    "- Given each sentence in the dataset, write the required functions to provide:\n",
    "    1. Subject, obects (direct and indirect)\n",
    "    2. Noun chunks\n",
    "    3. The head noun in each noun chunk\n",
    "    \n",
    "**Attention**: *print only the results of these functions by using the sentence \"I saw the man with a telescope\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6292d5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84838829",
   "metadata": {},
   "source": [
    "#### Q 2.2\n",
    "- Given a dependecy tree of a sentence and a segment of that sentence write the required functions that ouput the dependency subtree of that segment.\n",
    "\n",
    "**Attention**: *print only the results of these functions by using the sentence \"I saw the man with a telescope\" (the segment could be any e.g. \"saw the man\", \"a telescope\", etc.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d524e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "292e99ac",
   "metadata": {},
   "source": [
    "#### Q 2.3\n",
    "- Given a token in a sentence, write the required functions that output the dependency path from the root of the dependency tree to that given token.\n",
    "\n",
    "**Attention**: *print only the results of these functions by using the sentence \"I saw the man with a telescope\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0b1106",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3358779",
   "metadata": {},
   "source": [
    "### Task 3: Named Entity Recognition\n",
    "*Suggestion: use scikit-learn metric functions. See classification_report*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c820ad69",
   "metadata": {},
   "source": [
    "#### Q 3.1\n",
    "- Benchmark Spacy Named Entity Recognition model on the test set by:\n",
    "    1. Providing the list of categories in the dataset (person, organization, etc.)\n",
    "    2. Computing the overall accuracy on NER\n",
    "    3. Computing the performance of the Named Entity Recognition model for each category:\n",
    "        - Compute the perfomance at the token level (eg. B-Person, I-Person, B-Organization, I-Organization, O, etc.)\n",
    "        - Compute the performance at the entity level (eg. Person, Organization, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a051c93d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d669ee84",
   "metadata": {},
   "source": [
    "### Task 4: BONUS PART (extra mark for laude)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f56fc4f",
   "metadata": {},
   "source": [
    "#### Q 4.1\n",
    "- Modify NLTK Transition parser's Configuration calss to use better features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b182ed9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41ebf011",
   "metadata": {},
   "source": [
    "#### Q 4.2\n",
    "- Evaluate the features comparing performance to the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5177f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cfa4657c",
   "metadata": {},
   "source": [
    "#### Q 4.3\n",
    "- Replace SVM classifier with an alternative of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b94966",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
